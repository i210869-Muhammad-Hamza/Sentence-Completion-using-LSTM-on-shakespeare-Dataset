{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.17.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:31.508822Z","iopub.status.busy":"2024-09-28T17:24:31.508446Z","iopub.status.idle":"2024-09-28T17:24:33.371011Z","shell.execute_reply":"2024-09-28T17:24:33.370032Z","shell.execute_reply.started":"2024-09-28T17:24:31.508785Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Nutech\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","import string\n","\n","# Download stop words if not already downloaded\n","nltk.download('stopwords')\n","\n","# Function to read the text from a file\n","def read_text_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","    return text\n","\n","# Function to preprocess the text by removing stop words and punctuation\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    \n","    # Tokenize the text and remove punctuation\n","    words = text.split()\n","    words = [word.strip(string.punctuation).lower() for word in words]\n","    \n","    # Remove stop words\n","    filtered_text = [word for word in words if word not in stop_words and word.isalpha()]\n","    \n","    return ' '.join(filtered_text)\n","\n","# Example usage\n","file_path = 'alllines.txt'\n","\n","# First, read the text from the file\n","raw_text = read_text_file(file_path)\n","\n","# Then, preprocess the text\n","cleaned_text = preprocess_text(raw_text)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:33.374094Z","iopub.status.busy":"2024-09-28T17:24:33.373530Z","iopub.status.idle":"2024-09-28T17:24:42.908815Z","shell.execute_reply":"2024-09-28T17:24:42.908103Z","shell.execute_reply.started":"2024-09-28T17:24:33.374051Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n"]}],"source":["import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:42.910782Z","iopub.status.busy":"2024-09-28T17:24:42.910210Z","iopub.status.idle":"2024-09-28T17:24:42.924436Z","shell.execute_reply":"2024-09-28T17:24:42.917927Z","shell.execute_reply.started":"2024-09-28T17:24:42.910746Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'act scene london palace enter king henry lord john lancaster earl westmoreland sir walter blunt othe'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["cleaned_text[0:100]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:42.931178Z","iopub.status.busy":"2024-09-28T17:24:42.929785Z","iopub.status.idle":"2024-09-28T17:24:43.049517Z","shell.execute_reply":"2024-09-28T17:24:43.048719Z","shell.execute_reply.started":"2024-09-28T17:24:42.931125Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","# Function to tokenize words into integers\n","def tokenize_text_to_integers(text):\n","    # Tokenize the text into words\n","    words = text.split()\n","    \n","    # Create a vocabulary (word -> integer mapping)\n","    word_to_index = defaultdict(lambda: len(word_to_index))  # Automatically assign a new index to new words\n","    \n","    # Convert words to their corresponding integer tokens\n","    tokenized_text = [word_to_index[word] for word in words]\n","    \n","    return tokenized_text, dict(word_to_index)  # Returning both tokenized text and the vocabulary\n","\n","# Tokenize the cleaned text\n","tokenized_text, vocab = tokenize_text_to_integers(cleaned_text)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:43.052044Z","iopub.status.busy":"2024-09-28T17:24:43.051713Z","iopub.status.idle":"2024-09-28T17:24:43.743818Z","shell.execute_reply":"2024-09-28T17:24:43.743011Z","shell.execute_reply.started":"2024-09-28T17:24:43.052009Z"},"trusted":true},"outputs":[],"source":["def create_sequences(tokenized_text, seq_length):\n","    sequences = []\n","    targets = []\n","    \n","    # Loop over the tokenized text to create overlapping sequences\n","    for i in range(len(tokenized_text) - seq_length):\n","        # Extract the sequence of fixed length\n","        seq = tokenized_text[i:i + seq_length]\n","        # The target is the next word (after the sequence)\n","        target = tokenized_text[i + seq_length]\n","        \n","        sequences.append(seq)\n","        targets.append(target)\n","    \n","    return sequences, targets\n","\n","\n","# Generate sequences and corresponding targets\n","seq_length=5\n","sequences, targets = create_sequences(tokenized_text, seq_length)\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:43.745732Z","iopub.status.busy":"2024-09-28T17:24:43.745283Z","iopub.status.idle":"2024-09-28T17:24:43.750200Z","shell.execute_reply":"2024-09-28T17:24:43.749199Z","shell.execute_reply.started":"2024-09-28T17:24:43.745685Z"},"trusted":true},"outputs":[],"source":["vocab_size = len(vocab)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:43.751620Z","iopub.status.busy":"2024-09-28T17:24:43.751343Z","iopub.status.idle":"2024-09-28T17:24:43.760749Z","shell.execute_reply":"2024-09-28T17:24:43.759944Z","shell.execute_reply.started":"2024-09-28T17:24:43.751573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["21503\n"]}],"source":["print(vocab_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:43.762393Z","iopub.status.busy":"2024-09-28T17:24:43.761960Z","iopub.status.idle":"2024-09-28T17:24:43.770204Z","shell.execute_reply":"2024-09-28T17:24:43.769429Z","shell.execute_reply.started":"2024-09-28T17:24:43.762359Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["394593\n"]}],"source":["print(len(sequences))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["index_to_word = {v: k for k, v in vocab.items()}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:24:43.772106Z","iopub.status.busy":"2024-09-28T17:24:43.771738Z","iopub.status.idle":"2024-09-28T17:25:49.927212Z","shell.execute_reply":"2024-09-28T17:25:49.926250Z","shell.execute_reply.started":"2024-09-28T17:24:43.772063Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Function to prepare the input and target data for training\n","def prepare_training_data(sequences, targets, vocab_size):\n","    # Padding sequences to ensure uniform length\n","    X = np.array(sequences[0:10000])  # Already in correct sequence length\n","    y = to_categorical(targets[0:10000], num_classes=vocab_size)  # One-hot encode the targets\n","    return X, y\n","\n","# Build the LSTM model\n","def build_lstm_model(vocab_size, seq_length, embedding_dim, lstm_units):\n","    model = Sequential()\n","    \n","    # Embedding layer to represent each word as a dense vector of given embedding dimension\n","    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n","    \n","    # LSTM layer\n","    model.add(LSTM(lstm_units, return_sequences=False))\n","    \n","    # Dense output layer with softmax to predict the next word\n","    model.add(Dense(128, activation='relu'))\n","    \n","    model.add(Dense(256, activation='relu'))\n","\n","    model.add(Dense(512, activation='relu'))\n","\n","    \n","    model.add(Dense(vocab_size, activation='softmax'))\n","    \n","    # Compile the model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model\n","\n","# Train the model\n","def train_lstm_model(model, X, y, epochs, batch_size=64):\n","    with tf.device('/device:GPU:0'):    \n","        model.fit(X, y, epochs=epochs, batch_size=batch_size)\n","    \n","# Function to predict the next word\n","def predict_next_word(model, user_input, word_to_index, index_to_word, seq_length):\n","    # Convert user input into tokens (integers)\n","    tokenized_input = [word_to_index[word] for word in user_input if word in word_to_index]\n","    \n","    # Pad or truncate input to the required sequence length\n","    tokenized_input = pad_sequences([tokenized_input], maxlen=seq_length, padding='pre')\n","    \n","    # Predict the next word (returns probabilities for each word in the vocab)\n","    predicted_probabilities = model.predict(tokenized_input, verbose=0)[0]\n","    \n","    # Get the word index with the highest probability\n","    predicted_index = np.argmax(predicted_probabilities)\n","    \n","    # Return the corresponding word\n","    return index_to_word[predicted_index]\n","\n","# Example Workflow\n","\n","# Example tokenized text\n","#tokenized_text = [0, 1, 2, 3, 4, 5, 6, 7, 2, 3]  # Pre-tokenized example\n","#vocab = {'this': 0, 'is': 1, 'an': 2, 'example': 3, 'of': 4, 'tokenizing': 5, 'text': 6, 'into': 7, 'integers': 8}\n","index_to_word = {v: k for k, v in vocab.items()}  # Inverse mapping for predictions\n","\n","# Parameters\n","seq_length = 5\n","embedding_dim = 200\n","lstm_units = 1024\n","vocab_size = len(vocab)\n","\n","# Generate sequences and targets\n","sequences, targets = create_sequences(tokenized_text, seq_length)\n","\n","# Prepare data for training\n","X, y = prepare_training_data(sequences, targets, vocab_size)\n","\n","# Build the model\n","#lstm_model = build_lstm_model(vocab_size, seq_length, embedding_dim, lstm_units)\n","\n","# Train the model\n","#train_lstm_model(lstm_model, X, y, epochs=10)  # You can adjust the number of epochs\n","\n","# Prediction\n","#user_input = ['this', 'is', 'an', 'example', 'of']  # Example user input\n","#predicted_word = predict_next_word(lstm_model, user_input, vocab, index_to_word, seq_length)\n","\n","#print(f\"Predicted next word: {predicted_word}\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def predict_next_word(model, user_input, word_to_index, index_to_word, seq_length):\n","    # Convert user input into tokens (integers)\n","    tokenized_input = [word_to_index[word] for word in user_input if word in word_to_index]\n","    \n","    # Pad or truncate input to the required sequence length\n","    tokenized_input = pad_sequences([tokenized_input], maxlen=seq_length, padding='pre')\n","    \n","    # Predict the next word (returns probabilities for each word in the vocab)\n","    predicted_probabilities = model.predict(tokenized_input, verbose=0)[0]\n","    \n","    # Get the word index with the highest probability\n","    predicted_index = np.argmax(predicted_probabilities)\n","    \n","    # Return the corresponding word\n","    return index_to_word[predicted_index]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["loaded_model=tf.keras.models.load_model('new_model.h5')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T17:25:49.928923Z","iopub.status.busy":"2024-09-28T17:25:49.928584Z","iopub.status.idle":"2024-09-28T17:25:50.122664Z","shell.execute_reply":"2024-09-28T17:25:50.121676Z","shell.execute_reply.started":"2024-09-28T17:25:49.928862Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted next word: john\n"]}],"source":["# Prediction\n","user_input = ['Was', 'this', 'an ', 'answer', 'to']  # Example user input\n","predicted_word = predict_next_word(loaded_model, user_input, vocab, index_to_word, seq_length)\n","\n","print(f\"Predicted next word: {predicted_word}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-28 23:03:05.458 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 \n","  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n","  command:\n","\n","    streamlit run C:\\Users\\Nutech\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Session state does not function when running a script without `streamlit run`\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-09-28 23:03:05.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"]}],"source":["import streamlit as st\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","\n","# Load the trained model\n","#@st.cache_resource\n","st.title(\"Next Word Prediction with LSTM\")\n","st.write(\"Enter a sequence of 5 words, and the model will predict the next word.\")\n","\n","# User input\n","user_input = st.text_input(\"Enter 5 words separated by space:\")\n","\n","# Load the LSTM model\n","\n","# Define your word-to-index and index-to-word mappings\n","# For example:\n","#word_to_index = {'this': 0, 'is': 1, 'an': 2, 'example': 3, 'of': 4, 'tokenizing': 5, 'text': 6, 'into': 7, 'integers': 8}\n","#index_to_word = {v: k for k, v in word_to_index.items()}  # Inverse mapping for predictions\n","seq_length = 5  # Assuming you're using 5-word sequences\n","\n","if user_input:\n","    input_words = user_input.split()\n","    \n","    # Ensure exactly 5 words are input\n","    if len(input_words) == 5:\n","        predicted_word = predict_next_word(loaded_model, user_input, vocab, index_to_word, seq_length)\n","        st.write(f\"Next word predicted: {predicted_word}\")\n","        \n","        # Button to append the predicted word and allow continued text generation\n","        if st.button(\"Continue Prediction\"):\n","            input_words.append(predicted_word)\n","            st.text(f\"Updated sequence: {' '.join(input_words)}\")\n","    else:\n","        st.write(\"Please enter exactly 5 words.\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import json\n","\n","\n","\n","# Save to a JSON file\n","with open('variables.json', 'w') as f:\n","    json.dump({'vocab': vocab, 'index_to_word': index_to_word}, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5781454,"sourceId":9500097,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
